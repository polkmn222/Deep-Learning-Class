# -*- coding: utf-8 -*-
"""01)-03_어간 추출 및 표제어 추출.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u2Aiu-1UHbvy7TBJoAIO9WCSpDxEIAq0

# 03)어간 추출(Stemming) and 표제어 추출(Lemmatization)
"""

# # 정규표현식을 사용해서 특수문자를 제거
# import re
# # 소문자와 대문자가 아닌 것은 공백으로 대체한다.
# letters_only = re.sub('[^a-zA-Z]', ' ', example1.get_text())
# letters_only[:700]

import nltk
nltk.download('wordnet')
import nltk
nltk.download('omw-1.4')

"""### * 표제어 추출(Lammatization)"""

from nltk.stem import WordNetLemmatizer

### 객체화
lemmatizer = WordNetLemmatizer()

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
print('표제어 추출 전:', words)
print('표제어 추출 후:', [lemmatizer.lemmatize(word) for word in words])

lemmatizer.lemmatize('watched', 'v')
lemmatizer.lemmatize('has', 'v')

"""### * 어간 추출(Stemming)"""

import nltk
nltk.download('punkt')

sentences = "This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

### 포터 알고리즘 객체화
stemmer = PorterStemmer()

### word_tokenize를 통해 단어로 분리
tokenize_sentences = word_tokenize(sentences)
print('어간 추출 전:', tokenize_sentences)
print('어간 추출 후:', [stemmer.stem(word) for word in tokenize_sentences])

## Porter 알고리즘의 기준 혹은 규칙은 마틴 포터의 홈페이지에서
## 확인이 가능합니다

from nltk.stem import PorterStemmer
from nltk.stem import LancasterStemmer  # 요 기준으로 한 번 다른 단어를 stemming

## 한국어의 어간과 어미
잡다

-> 잡/어간 + 다/어미

## stemming 적용
# 어간추출 전의 words
words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']

porter_stemmer = PorterStemmer()
lanca_stemmer = LancasterStemmer()

print('어간 추출 전:', words)
print('포터 스테머 적용:', [porter_stemmer.stem(word) for word in words])
print('랭커스터 스테머 적용:', [lanca_stemmer.stem(word) for word in words])

# end of files