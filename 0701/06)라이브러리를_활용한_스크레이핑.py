# -*- coding: utf-8 -*-
"""06)라이브러리를_활용한_스크레이핑.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bgcEreTnxaffYzR_NXssdC5FsLP9tubZ

서드파티 라이브러리를 활용하면 지금까지 설명한 방법보다 간단히 크롤링/스크레이핑이 가능하다.


    서드파티 라이브러리 :: 표준 라이브러리와는 별도로 다른 사람들이 직접 만들어 공개한 라이브러리를 의미

### 06)-1 라이브러리 설치

pip install <라이브러리 이름>

pip install로 설치 시에는 같은 버전의 라이브러리를 여러 개 동시에 설치할 수 없다. 여러 개의 버전을 설치하고 싶은 경우에는 가상환경을 나누어 설치해야 한다.

### 06)-2 웹 페이지 간단하게 추출하기

표준라이브러리 urllib을 사용하는 것에 비해 Requests를 사용하면 매우 쉽게 웹페이지의 내용을 추출할 수 있다. Requests의 캐치프레이즈는 "HTTP for Humans"이다.

urllib의 경우 GET, POST같은 요청을 처리시에는 간단하나, HTTP 헤더 추가 또는 Basic 인증 등의 조금 더 나아간 처리를 할 때 굉장히 복잡해진다. 하지만 Requests 모듈을 사용하면 이러한 것들도 쉽게 할 수 있는 인터페이스를 제공해준다.
"""

!pip install requests

import requests # requests 호출
r = requests.get('https://hanbit.co.kr/') # get() 함수로 웹 페이지를 추출
print(type(r))                                   # Response [200]이라는 용어는 연결
print(r.status_code)

r.headers['content-type'] # headers 속성으로 HTTP 헤더를 딕셔너리로 추출

r.encoding # encoding 속성으로 HTTP 헤더를 기반으로 인코딩을 추출

r.text # text 속성으로 str 자료형으로 디코딩된 응답 본문 추출이 가능

# bytes 자료형의 응답 본문을 추출하고 싶다면? 
r.content[:700]

"""Response 객체의 text 속성을 기반으로 유니코드 문자열을 쉽게 추출이 가능하다 HTTP 헤더에서 응답 본문의 인코딩 방식을 추출하고, str 자료형으로 디코딩하기만 하면 된다.

만약 응답 본문이 gzip형식 또는 Deflate 형식으로 압축돼 있는 경우에도 자동으로 해석해 주므로 압축관련 내용을 따로 신경쓸 필요는 없다.
"""

# requests에는 post(), put(), delete(), head(), options()함수가 있으며,
# 각각 HTTP 메서드의 POST, PUT, DELETE, HEAD, OPTIONS에 대응

# POST 메서드로 전송
# 키워드 매개변수 data에 딕셔너리를 지정하면 HTML 입력 양식처럼 전송.
r = requests.post('http://httpbin.org/post', data={'key1':'value1'})

# 요청에 추가할 HTTP 헤더는 키워드 매개변수 headers에 딕셔너리로 지정
r = requests.get('http://httpbin.org/get',
                 headers = {'user-agent': 'my-crawler/1.0 (+foo@example.com)'})

# # Basic 인증은 키워드 매개변수 auth로 지정
# r = requests.get('https://api.github.com/user',
#                  auth = ('<GitHub의 사용자 ID>', '<GitHub의 비밀번호>' ))

# URL 매개변수는 키워드 매개변수 params로도 지정 가능
r = requests.get('http://httpbin.org/get', data={'key1':'value1'})

"""여러 개의 페이지를 연속으로 크롤링 시 Session 객체를 활용하면 효과적이다. Session 객체를 사용하면 HTTP 헤더 또는 Basic 인증 등의 설정을 한 번만 수행한 후 여러 번 재사용 가능하다. 또한 Cookie도 자동으로 지원된다.

참고로 Session 객체를 사용해 같은 웹사이트에 여러번 요청할 대에는 HTTP Keep-Alive 방식을 활용한다. 한 번 확립한 TCP 요청을 계속 활용하므로 오버헤드가 되는 TCP 커넥션 확립 처리를 줄일 수 있는 장점이 기대된다.

특히 https://로 시작되는 URL에 요청을 보낼 때 TCP 네트워크 확립에 암호화를 위한 TLS/SSL 핸드쉐이크를 하게 되는데, 부하가 많이 걸리므로
위에서 언급한 HTTP Keep-Alive 방식으로 서버측 부하도 줄일 수 있다.
"""

# HTTP 헤더를 여러 번 사용할 시 session 함수 활용
s = requests.Session()
# s.headers.update(headers = {'user-agent': 'my-crawler/1.0 (+foo@example.com)'})

# 위의 cell에서 설정한 {'user-agent': 'my-crawler/1.0 (+foo@example.com)'}
# 와 같은 dict 형태이므로, 일단 주석처리

r = s.get('https://naver.com')
r = s.get('https://www.daum.net/')

"""### HTML 스크레이핑

HTML 스크레이핑 할 때 널리 사용되는 라이브러리들의 경우에는 *** XPath와 CSS 선택자***를 활용한다. XPath와 CSS 선택자에 대한 설명과 구체적인 라이브러리로 ***lxml과 Beautiful Soup***을 사용하는 방법 또한 살펴보자.

*    lxml은 C언어로 작성된 XML처리와 관련된 유명한 라이브러리인 libxml2와 libxslt의 파이썬 파인딩이다. libxml2와 libxslt은 C언어 기반이므로 속도가 매우 빠르다.

    또한 단순히 C언어 라이브러리를 wrapping 했다는 것 뿐 만 아니라 파이썬에서 사용하기 쉬운 API를 제공한다는 것 또한 특징이다. 

    단점을 굳이 꼽자면 기능이 너무 많기에 처음 사용하는 사용자의 적응문제 정도가 있다.

*    Beautiful Soup은 간단하고 이해하기 쉬운 직관적인 API를 활용해 데이터를 추출할 수 있다는 것이 특징이다. 과거부터 굉장히 인기있고 자주 쓰이는 라이브러리 중 하나이며, 내부적으로 사용되는 파서(parser)를 목적에 맞게 변경도 가능하다.

*    pyquery는 자바스크립트(JSP)의 라이브러리인 JQuery와 같은 인터페이스로 스크레이핑(Scraping)할 수 있게 해주는 것이 특징이다. jQuery의 $함수에 CSS 선택자를 매개변수로 지정하는 것과 비슷한 형태로 사용하므로 jQuery에 익숙한 사용자라면 매우 친숙할 것이다. 참고로 pyquery의 경우 내부적으로는 lxml을 활용한다.

스크레이핑을 위한 라이브러리의 종류는 매우 많으나, 초보자의 경우 lxml을 경험해보기를 권한다. 그 이유는 Scraping이라는 처리는 꽤나 무거운 처리이므로 빠른 처리가 가능한 C기반의 라이브러리를 활용하는 것이 권장되는데 lxml이 그렇기 때문이다.

### XPath와 CSS 선택자의 개념

* XPath(XML Path Language) - XML의 특정 요소를 지정할 때 사용되는 언어이다. 예를 들어, //body/h1이라고 지정하면 body 요소의 직접적인 자식중에 h1 태그를 선택하게 됨



* CSS 선택자 - CSS로 요소를 디자인할 때 사용하는 표기 방법이다. 예를 들어, body > h1이라고 표기하면 body 요소의 직접적인 자식중에 h1 태그를 선택하게 됨

|대상요소|XPath|CSS 선택자|
|-----|-----|-----|
|title요소|//title| title|
|body 요소의 후손 중 h1 요소|//body//h1|body h1|
|body 요소의 자식 중 h1 요소|//body/h1|body > h1|
|body 요소의 내부의 모든 자식요소|//body/* | body > *|
|Id 속성이 "main"인 요소|id("main") 또는 //*[@id="main"] | #main|
|class 속성으로 "active"를 포함하고 있는 li 요소|//li[@class and contains(concat(' ', normalize-space(@class),' '), 'active')]|li.active
|type 속성이 "text"인 input 요소|//input[@type="text"]|input[type="text"]
|href 속성이 "http://"로 시작하는 a 요소|//a[starts-with(@href, "http://")]|a[href^="http://"]
|src 속성이 ".jpg"로 끝나는 img 요소|//img[ends-with(@src, ".jpg"]| img[src$=".jpg"]
|요소의 내부에 "개요"라는 텍스트 노드가 포함돼 있는 h2 요소|//h2[contains(.,"개요")]|h2:contains("개요") ※ cssselect의 독자적인 구현이므로 표준 CSS 사양은 아니다
|요소의 바로 아래에 "개요"라는 텍스트 노드가 포함돼 있는 h2 요소|//h2[text() = "개요")]|※ CSS 선택자로는 표현 불가능
"""

# lxml로 스크레이핑 시도 
!pip install lxml

!pip install cssselect

## lxml 사용법
import lxml.html

# parse 함수로 파일 경로를 지정 가능하다
# tree = lxml.html.parse('/content/drive/MyDrive/Colab Notebooks/98_crawling_scraping/full_book_list.html')


# parse()함수로 URL 지정도 가능하지만, 추출 시 미세한 설정을 따로 할 수 없으므로 추천되진 않는다.
tree = lxml.html.parse('http://example.com')

# 파일 객체를 지정하여 parsing 할 수도 있다.
from urllib.request import urlopen
tree = lxml.html.parse(urlopen('http://example.com'))

type(tree) # parsing하면 ElementTree객체로 추출된다

html = tree.getroot() #getroot()메서드로 html 루트 요소의 HtmlElement 객체를 추출할 수 잇다.
type(html)

# lxml로 스크레이핑
import lxml.html

# html read and make HtmlElement by method getroot()
# tree = lxml.html.parse('/content/drive/MyDrive/Colab Notebooks/98_crawling_scraping/full_book_list.html')
html = tree.getroot()

# cssselect()메서드로 a요소의 리스트를 추출하고 for문을 수행한다.
for a in html.cssselect('a'):
    # href 속성과 글자를 추출한다.
    print(a.get('href'), a.text)

"""### Beautiful Soup로 스크레이핑하기

Beautiful Soup은 기억하기 쉬운 단순한 API가 특징인 스크레이핑 라이브러리이다. 목적에 따라 파서(parser)를 선택해서 사용할 수 있다.

참고로 Beautiful Soup은 2012년에 version 4가 공개된 이후 크게 변경되지 않고 있다. 기본적인 API는 변경되지 않았지만 내부적인 패키지명 또는 모듈 이름 등이 조금 수정되었다.
"""

! pip install beautifulsoup4

from bs4 import BeautifulSoup # bs4 모듈에서 BeautifulSoup 클래스를 호출한다

# 첫 번째 매개변수에 file object를 지정해 BeautifulSoup 객체를 생성 
# BeautifulSoup()에는 파일 이름 또는 URL을 지정할 수 없다.
# 두 번째 매개변수에는 parser의 종류를 지정 가능하다

with open('/content/drive/MyDrive/Colab Notebooks/98_crawling_scraping/full_book_list.html') as f:
    soup = BeautifulSoup(f,'html.parser')


# find_all()메서드로 a 요소를 추출하고 for문 적용
for a in soup.find_all('a'):
    # href 속성과 글자 추출
    print(a.get('href'), a.text)

## parsing 된 데이터 soup
soup

## soup.h1처럼 태그 이름을 가진 속성으로 h1 요소를 추출할 수 있다.
soup.h1
print(type(soup.h1)) #요소는 Tag 객체이다.

# name 속성으로 태그 이름 추출 가능
print(soup.h1.name)

# string 속성은 str을 상속한 NavigableString 객체
print(soup.h1.string)
print(type(soup.h1.string))

# text 속성은 요소 내부 모든 문자열을 결합하여 문자열 추출
print(soup.ul.text)
print(type(soup.ul.text)) # 자료형은 str

print(soup.h1.get('id'))

"""### feedparser를 활용하여 RSS 스크레이핑"""

!pip install feedparser

import feedparser

d= feedparser.parse('https://www.aladin.co.kr/rss/special_new/351')

# 항목 조회
for entry in d.entries:
    print('이름:', entry.title)
    print('타이틀:', entry.link)
    print()

# end of file